import yfinance as yf
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error

winners = ['ASHG.TA', 'BEZQ.TA', 'ENOG.TA', 'ESLT.TA',
           'ICL.TA', 'KEN.TA', 'LUMI.TA', 'TSEM.TA']
df = pd.read_csv('data/winner.csv', index_col=0, parse_dates=True)
stock = df[['ASHG.TA']].copy()
stock_future = stock.tail(1)
print(stock_future)

scaler = MinMaxScaler(feature_range=(-1, 1))
df = scaler.fit_transform(df.values.reshape(-1, 1))


def load_data(stock, window_size):
    data_raw = stock
    data = []

    for index in range(len(data_raw) - window_size):
        data.append(data_raw[index: index + window_size])

    data = np.array(data)

    x = data[:, :-1, :]

    y = data[:, -1, :]

    return [x, y]


ws = 4
x, y = load_data(stock, ws)
print('x.shape = ', x.shape)
print('y.shape = ', y.shape)

x = torch.from_numpy(x).type(torch.Tensor)

y = torch.from_numpy(y).type(torch.Tensor)


input_dim = 1
hidden_dim = 128
num_layers = 2
output_dim = 1


class LSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):
        super(LSTM, self).__init__()

        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        self.lstm = nn.LSTM(input_dim, hidden_dim,
                            num_layers, batch_first=True)

        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):

        h0 = torch.zeros(self.num_layers, x.size(
            0), self.hidden_dim).requires_grad_()
        c0 = torch.zeros(self.num_layers, x.size(
            0), self.hidden_dim).requires_grad_()
        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))
        out = self.fc(out[:, -1, :])
        return out


torch.manual_seed(68)

model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim,
             output_dim=output_dim, num_layers=num_layers)

criterion = torch.nn.MSELoss()

optimiser = torch.optim.Adam(model.parameters(), lr=0.01)

num_epochs = 901
hist = np.zeros(num_epochs)


for t in range(num_epochs):
    # Initialise hidden state
    # Don't do this if you want your LSTM to be stateful
    #model.hidden = model.init_hidden()

    # Forward pass
    y_train_pred = model(x)

    loss = criterion(y_train_pred, y)
    if t % 100 == 0 and t != 0:
        print("Epoch ", t, "MSE: ", loss.item())
    hist[t] = loss.item()

    # Zero out gradient, else they will accumulate between epochs
    optimiser.zero_grad()

    # Backward pass
    loss.backward()

    # Update parameters
    optimiser.step()


y_pred = model(x)
y_pred = y_pred.detach().numpy()
y_pred = np.expand_dims(y_pred, axis=0)
window_size = 4
future = 1
L = len(y)

preds = y_pred[-window_size:].tolist()

model.eval()
for i in range(future):
    seq = torch.FloatTensor(y_pred[-3:])
    with torch.no_grad():

        preds.append(model(seq).item())

ASHG = scaler.inverse_transform(np.array(preds[-1]).reshape(-1, 1))
ASHG = ASHG.astype(float)

print(ASHG)
